{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch # 确保torch也被导入\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a6e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载YOLOv8n模型...\n",
      "模型加载完毕。\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 加载预训练的YOLO模型 ---\n",
    "print(\"正在加载YOLOv8n模型...\")\n",
    "model = YOLO('backend/resource/yolo11n.pt')\n",
    "print(\"模型加载完毕。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff4cfdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载图片: backend/resource/bus.jpg\n",
      "图片尺寸: 810x1080\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 加载图像 ---\n",
    "image_path = 'backend/resource/bus.jpg'\n",
    "try:\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"错误：无法加载图片，请检查路径是否正确: {image_path}\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"加载图片时出错: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"成功加载图片: {image_path}\")\n",
    "# --- 新增：获取图片尺寸 ---\n",
    "# img.shape 返回一个元组 (height, width, channels)\n",
    "height, width, _ = img.shape\n",
    "image_dimensions = {\"width\": width, \"height\": height}\n",
    "\n",
    "print(f\"图片尺寸: {width}x{height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b18437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在对图像进行目标检测...\n",
      "\n",
      "0: 640x480 4 persons, 1 bus, 152.5ms\n",
      "Speed: 8.0ms preprocess, 152.5ms inference, 9.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "检测完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 执行检测 ---\n",
    "print(\"正在对图像进行目标检测...\")\n",
    "results = model(img)\n",
    "print(\"检测完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d893ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 提取到的完整场景数据 (Python Dict) ---\n",
      "{\n",
      "  \"image_dimensions\": {\n",
      "    \"width\": 810,\n",
      "    \"height\": 1080\n",
      "  },\n",
      "  \"detected_objects\": [\n",
      "    {\n",
      "      \"class\": \"bus\",\n",
      "      \"confidence\": 0.9402,\n",
      "      \"bounding_box\": [\n",
      "        3,\n",
      "        229,\n",
      "        796,\n",
      "        728\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"class\": \"person\",\n",
      "      \"confidence\": 0.8882,\n",
      "      \"bounding_box\": [\n",
      "        671,\n",
      "        394,\n",
      "        809,\n",
      "        878\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"class\": \"person\",\n",
      "      \"confidence\": 0.8783,\n",
      "      \"bounding_box\": [\n",
      "        47,\n",
      "        399,\n",
      "        239,\n",
      "        904\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"class\": \"person\",\n",
      "      \"confidence\": 0.8558,\n",
      "      \"bounding_box\": [\n",
      "        223,\n",
      "        408,\n",
      "        344,\n",
      "        860\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"class\": \"person\",\n",
      "      \"confidence\": 0.6219,\n",
      "      \"bounding_box\": [\n",
      "        0,\n",
      "        556,\n",
      "        68,\n",
      "        872\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 提取结构化数据 (整合为单一JSON对象) ---\n",
    "# 这是项目目标[6]中提到的提取关键特征的一部分\n",
    "detected_objects_list = []\n",
    "for box in results[0].boxes:\n",
    "    class_id = int(box.cls[0])\n",
    "    confidence = float(box.conf[0])\n",
    "    class_name = model.names[class_id]\n",
    "    bounding_box = [int(coord) for coord in box.xyxy[0].tolist()]\n",
    "    \n",
    "    detected_objects_list.append({\n",
    "        \"class\": class_name,\n",
    "        \"confidence\": round(confidence, 4),\n",
    "        \"bounding_box\": bounding_box\n",
    "    })\n",
    "\n",
    "# 创建一个顶层的数据结构来包含所有信息\n",
    "scene_data = {\n",
    "    \"image_dimensions\": image_dimensions,\n",
    "    \"detected_objects\": detected_objects_list\n",
    "}\n",
    "\n",
    "print(\"\\n--- 提取到的完整场景数据 (Python Dict) ---\")\n",
    "# 使用json.dumps来美化打印输出\n",
    "json_data = json.dumps(scene_data, indent=2)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6ca32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an AI assistant that describes images based on a list of detected objects provided in JSON format.\\n\\nFirst, understand the data structure I will provide. The data will be a JSON object with the following schema:\\n- \"image_properties\": An object containing the overall image dimensions.\\n  - \"width\": (integer) The total width of the image in pixels.\\n  - \"height\": (integer) The total height of the image in pixels.\\n- \"detected_objects\": A list of objects found in the image. Each object in this list has the following structure:\\n  - \"class\": (string) The name of the detected object class.\\n  - \"confidence\": (float) The model\\'s confidence in the detection, from 0.0 to 1.0.\\n  - \"bounding_box\": A list of four integers [x_center , y_center, width, height]\\n\\nNow, analyze the following specific scene data based on the schema described above.\\nDetected objects data:\\n{\\n  \"image_dimensions\": {\\n    \"width\": 810,\\n    \"height\": 1080\\n  },\\n  \"detected_objects\": [\\n    {\\n      \"class\": \"bus\",\\n      \"confidence\": 0.9402,\\n      \"bounding_box\": [\\n        3,\\n        229,\\n        796,\\n        728\\n      ]\\n    },\\n    {\\n      \"class\": \"person\",\\n      \"confidence\": 0.8882,\\n      \"bounding_box\": [\\n        671,\\n        394,\\n        809,\\n        878\\n      ]\\n    },\\n    {\\n      \"class\": \"person\",\\n      \"confidence\": 0.8783,\\n      \"bounding_box\": [\\n        47,\\n        399,\\n        239,\\n        904\\n      ]\\n    },\\n    {\\n      \"class\": \"person\",\\n      \"confidence\": 0.8558,\\n      \"bounding_box\": [\\n        223,\\n        408,\\n        344,\\n        860\\n      ]\\n    },\\n    {\\n      \"class\": \"person\",\\n      \"confidence\": 0.6219,\\n      \"bounding_box\": [\\n        0,\\n        556,\\n        68,\\n        872\\n      ]\\n    }\\n  ]\\n}\\nBased on this structured data, please generate a concise and natural language description of the scene. Try to infer relationships between objects and their general locations (e.g., \"left side\", \"center\", \"behind another object\") rather than just listing the raw bounding box coordinates. Focus on creating a human-like, coherent description of what the image likely contains.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 5. 创建LLM提示 ---\n",
    "# 这是项目目标[7]中提到的关键一步：动态格式化数据为提示\n",
    "def create_prompt_with_json(json_data):\n",
    "    # --- 新增：数据结构定义 ---\n",
    "    schema_description = \"\"\"\n",
    "First, understand the data structure I will provide. The data will be a JSON object with the following schema:\n",
    "- \"image_properties\": An object containing the overall image dimensions.\n",
    "  - \"width\": (integer) The total width of the image in pixels.\n",
    "  - \"height\": (integer) The total height of the image in pixels.\n",
    "- \"detected_objects\": A list of objects found in the image. Each object in this list has the following structure:\n",
    "  - \"class\": (string) The name of the detected object class.\n",
    "  - \"confidence\": (float) The model's confidence in the detection, from 0.0 to 1.0.\n",
    "  - \"bounding_box\": A list of four integers [x_center , y_center, width, height]\n",
    "\"\"\"\n",
    "    \n",
    "    # 构建最终的提示。这部分可以根据需要灵活修改\n",
    "    # 它包含一个角色指令、原始数据和具体任务\n",
    "    prompt = f\"\"\"You are an AI assistant that describes images based on a list of detected objects provided in JSON format.\n",
    "{schema_description}\n",
    "Now, analyze the following specific scene data based on the schema described above.\n",
    "Detected objects data:\n",
    "{json_data}\n",
    "Based on this structured data, please generate a concise and natural language description of the scene. Try to infer relationships between objects and their general locations (e.g., \"left side\", \"center\", \"behind another object\") rather than just listing the raw bounding box coordinates. Focus on creating a human-like, coherent description of what the image likely contains.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "prompt = create_prompt_with_json(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a7d32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mollama\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_local_llm_description\u001b[39m(prompt):\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# 将 'YOUR_MODEL_NAME' 替换为 `ollama list` 中显示的模型名称\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# 例如: 'deepseek-llm:8b'\u001b[39;00m\n\u001b[32m      7\u001b[39m     model_name = \u001b[33m'\u001b[39m\u001b[33mdeepseek-r1:8b\u001b[39m\u001b[33m'\u001b[39m \n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "# Ollama / DeepSeek Configuration\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "DEEPSEEK_MODEL_NAME = \"deepseek-r1:8b\" # 默认使用 \"deepseek-llm\"，请根据 `ollama list` 的结果修改\n",
    "\n",
    "def get_local_llm_description(prompt):\n",
    "\n",
    "    # 将 'YOUR_MODEL_NAME' 替换为 `ollama list` 中显示的模型名称\n",
    "    # 例如: 'deepseek-llm:8b'\n",
    "    model_name = DEEPSEEK_MODEL_NAME\n",
    "    print(f\"--- 正在发送提示到本地Ollama模型: {model_name} ---\")\n",
    "\n",
    "    try:\n",
    "        # 使用 ollama.chat 与本地模型交互\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ]\n",
    "        )\n",
    "        print(\"--- 已从本地模型接收到响应 ---\")\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"调用本地Ollama模型时出错: {e}\")\n",
    "        return f\"Error interacting with local Ollama model: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4c64ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (可选) 如果你还想看到图片，可以取消下面代码的注释\n",
    "annotated_frame = results[0].plot()\n",
    "cv2.imshow(\"YOLOv8 Detection\", annotated_frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image2text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
