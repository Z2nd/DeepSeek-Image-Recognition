{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9d0c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "Selected Operation Mode: camera\n",
      "Using YOLO model: yolov8n.pt\n",
      "Ollama API URL: http://localhost:11434/api/generate\n",
      "Attempting to use DeepSeek model: deepseek-r1:8b\n",
      "YOLO model 'yolov8n.pt' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import requests\n",
    "import json\n",
    "from PIL import Image # For displaying images in Notebook\n",
    "import numpy as np\n",
    "import os\n",
    "import time # For potential delays if needed\n",
    "\n",
    "# --- Configuration ---\n",
    "# CHOOSE OPERATION MODE: \"file\" or \"camera\"\n",
    "# OPERATION_MODE = \"file\"  # <<<< CHANGE THIS TO \"camera\" TO USE CAMERA MODE\n",
    "OPERATION_MODE = \"camera\"\n",
    "\n",
    "# YOLO Configuration\n",
    "YOLO_MODEL_NAME = 'yolov8n.pt'\n",
    "\n",
    "# Ollama / DeepSeek Configuration\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "DEEPSEEK_MODEL_NAME = \"deepseek-r1:8b\" # 默认使用 \"deepseek-llm\"，请根据 `ollama list` 的结果修改\n",
    "\n",
    "# Input Image Path (used only if OPERATION_MODE is \"file\")\n",
    "INPUT_IMAGE_PATH = \"bus.jpg\" # 例如: \"test_images/bus.jpg\"\n",
    "\n",
    "# Camera Configuration (used only if OPERATION_MODE is \"camera\")\n",
    "CAMERA_INDEX = 0 # 0 for default camera, change if you have multiple cameras\n",
    "\n",
    "# --- Sanity Checks ---\n",
    "print(\"Libraries imported.\")\n",
    "print(f\"Selected Operation Mode: {OPERATION_MODE}\")\n",
    "print(f\"Using YOLO model: {YOLO_MODEL_NAME}\")\n",
    "print(f\"Ollama API URL: {OLLAMA_API_URL}\")\n",
    "print(f\"Attempting to use DeepSeek model: {DEEPSEEK_MODEL_NAME}\")\n",
    "\n",
    "if DEEPSEEK_MODEL_NAME == \"your-deepseek-model-name\": # Default placeholder check\n",
    "    print(\"\\n⚠️ WARNING: 'DEEPSEEK_MODEL_NAME' is set to a placeholder. Please update it with your actual model name from Ollama (run `ollama list`).\")\n",
    "\n",
    "if OPERATION_MODE == \"file\":\n",
    "    if not os.path.exists(INPUT_IMAGE_PATH) and INPUT_IMAGE_PATH != \"path/to/your/image.jpg\":\n",
    "        print(f\"\\n⚠️ WARNING: Input image not found at '{INPUT_IMAGE_PATH}'. Please check the path.\")\n",
    "    elif INPUT_IMAGE_PATH == \"path/to/your/image.jpg\":\n",
    "         print(f\"\\n⚠️ WARNING: Please update 'INPUT_IMAGE_PATH' to your desired image for file mode.\")\n",
    "elif OPERATION_MODE not in [\"file\", \"camera\"]:\n",
    "    print(f\"\\n⚠️ ERROR: Invalid 'OPERATION_MODE' ('{OPERATION_MODE}'). Choose 'file' or 'camera'.\")\n",
    "\n",
    "\n",
    "# Load YOLO Model (common for both modes)\n",
    "try:\n",
    "    yolo_model = YOLO(YOLO_MODEL_NAME)\n",
    "    print(f\"YOLO model '{YOLO_MODEL_NAME}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YOLO model: {e}\")\n",
    "    yolo_model = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e724cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO detection helper function defined.\n"
     ]
    }
   ],
   "source": [
    "def detect_objects_yolo(image_np, model):\n",
    "    \"\"\"\n",
    "    Performs object detection using YOLO model.\n",
    "    Args:\n",
    "        image_np (numpy.ndarray): Image in NumPy array format (BGR).\n",
    "        model (ultralytics.YOLO): Loaded YOLO model.\n",
    "    Returns:\n",
    "        tuple: (list of detections, annotated_image_np)\n",
    "               Each detection is a dict: {'class_name': str, 'confidence': float, 'bbox': [x1, y1, x2, y2]}\n",
    "               annotated_image_np is the image with detections drawn on it.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(\"YOLO model not loaded. Skipping detection.\")\n",
    "        return [], image_np\n",
    "\n",
    "    detections_list = []\n",
    "    try:\n",
    "        results = model(image_np, verbose=False)  # verbose=False to reduce console output\n",
    "\n",
    "        # Assuming results[0] contains detections for the first image\n",
    "        if results and results[0]:\n",
    "            annotated_image = results[0].plot() # This returns a BGR NumPy array\n",
    "            boxes = results[0].boxes\n",
    "            names = results[0].names # Class names\n",
    "\n",
    "            for box in boxes:\n",
    "                class_id = int(box.cls[0])\n",
    "                class_name = names[class_id]\n",
    "                confidence = float(box.conf[0])\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                detections_list.append({\n",
    "                    'class_name': class_name,\n",
    "                    'confidence': confidence,\n",
    "                    'bbox': [x1, y1, x2, y2]\n",
    "                })\n",
    "        else:\n",
    "            annotated_image = image_np # Return original if no results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during YOLO detection: {e}\")\n",
    "        return [], image_np # Return original image in case of error\n",
    "\n",
    "    return detections_list, annotated_image\n",
    "\n",
    "print(\"YOLO detection helper function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68bee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection formatting (JSON) helper function defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Helper Function - Format Detections as JSON for LLM (Step C)\n",
    "def format_detections_as_json_for_llm(detections):\n",
    "    \"\"\"\n",
    "    Converts YOLO detections (list of dicts) into a JSON string for an LLM.\n",
    "    Args:\n",
    "        detections (list): List of detection dicts from detect_objects_yolo.\n",
    "                           Each dict: {'class_name': str, 'confidence': float, 'bbox': [x1, y1, x2, y2]}\n",
    "    Returns:\n",
    "        str: A JSON string representation of the detections, or a message if no detections.\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return \"No objects were detected in the image.\"\n",
    "    \n",
    "    # Convert the list of detection dictionaries to a JSON string\n",
    "    # indent=2 makes the JSON string more readable if printed or logged.\n",
    "    return json.dumps(detections, indent=2)\n",
    "\n",
    "print(\"Detection formatting (JSON) helper function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d549d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek text generation helper function defined.\n"
     ]
    }
   ],
   "source": [
    "def generate_text_with_deepseek(prompt_text, model_name=DEEPSEEK_MODEL_NAME, ollama_url=OLLAMA_API_URL):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a DeepSeek model via Ollama and returns the response.\n",
    "    \"\"\"\n",
    "    if model_name == \"your-deepseek-model-name\":\n",
    "        return \"Error: DeepSeek model name not configured. Please update 'DEEPSEEK_MODEL_NAME' in Cell 1.\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt_text,\n",
    "        \"stream\": False  # Get the full response at once\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    print(f\"\\nSending prompt to DeepSeek model '{model_name}'...\")\n",
    "    try:\n",
    "        response = requests.post(ollama_url, data=json.dumps(payload), headers=headers, timeout=180) # Increased timeout\n",
    "        response.raise_for_status()\n",
    "        response_data = response.json()\n",
    "        if \"response\" in response_data:\n",
    "            return response_data[\"response\"].strip()\n",
    "        else:\n",
    "            return f\"Error: 'response' key not found in Ollama API return. Full response: {response_data}\"\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return f\"Error: Could not connect to Ollama at {ollama_url}. Is Ollama running?\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return f\"Error: Request to Ollama timed out. The model might be taking too long or the service is unresponsive.\"\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return f\"Error: HTTP error from Ollama: {e}. Response: {e.response.text if e.response else 'No response body'}\"\n",
    "    except json.JSONDecodeError:\n",
    "        return f\"Error: Could not decode JSON response from Ollama. Response text: {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: An unexpected error occurred while querying DeepSeek: {e}\"\n",
    "\n",
    "print(\"DeepSeek text generation helper function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c17a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running in CAMERA mode (Camera Index: 0) ---\n",
      "Camera opened successfully.\n",
      "Instructions: Focus the OpenCV window. Press 'c' to capture and process the current frame. Press 'q' to quit.\n",
      "\n",
      "Waiting for your command... (Focus OpenCV window: Press 'c' to capture, 'q' to quit)\n",
      "Capture command received.\n",
      "\n",
      "--- Processing new frame from camera ---\n",
      "Frame dimensions: Width=640, Height=480\n",
      "\n",
      "--- Step B: Running YOLO Object Detection ---\n",
      "Detected 9 objects.\n",
      "\n",
      "--- Annotated Frame (YOLO Detections) ---\n",
      "\n",
      "--- Step C: Formatting Detections as JSON for LLM ---\n",
      "\n",
      "--- Step D: Generating Text with DeepSeek ---\n",
      "\n",
      "Sending prompt to DeepSeek model 'deepseek-r1:8b'...\n",
      "Camera mode interrupted by user (Ctrl+C).\n",
      "Camera released.\n",
      "OpenCV windows closed.\n"
     ]
    }
   ],
   "source": [
    "# Ensure helper functions from Cell 2, 3, 4 are defined before running this cell.\n",
    "\n",
    "# --- Preliminary Checks ---\n",
    "if yolo_model is None:\n",
    "    print(\"Skipping workflow: YOLO model failed to load. Please check Cell 1.\")\n",
    "elif DEEPSEEK_MODEL_NAME == \"your-deepseek-model-name\" or not DEEPSEEK_MODEL_NAME: # Check placeholder or empty\n",
    "    print(\"Skipping workflow: DeepSeek model name not configured correctly. Please check Cell 1.\")\n",
    "elif OPERATION_MODE not in [\"file\", \"camera\"]:\n",
    "    print(f\"Skipping workflow: Invalid 'OPERATION_MODE' ('{OPERATION_MODE}') in Cell 1. Choose 'file' or 'camera'.\")\n",
    "else:\n",
    "    # --- FILE MODE ---\n",
    "    if OPERATION_MODE == \"file\":\n",
    "        print(f\"--- Running in FILE mode for image: {INPUT_IMAGE_PATH} ---\")\n",
    "        if not os.path.exists(INPUT_IMAGE_PATH) or INPUT_IMAGE_PATH == \"path/to/your/image.jpg\":\n",
    "            print(f\"Skipping file mode: Input image path '{INPUT_IMAGE_PATH}' is invalid or not set. Please check Cell 1.\")\n",
    "        else:\n",
    "            try:\n",
    "                input_image_bgr = cv2.imread(INPUT_IMAGE_PATH)\n",
    "                if input_image_bgr is None:\n",
    "                    raise FileNotFoundError(f\"Could not read image at {INPUT_IMAGE_PATH}\")\n",
    "\n",
    "                print(\"\\n--- Original Image ---\")\n",
    "                input_image_rgb = cv2.cvtColor(input_image_bgr, cv2.COLOR_BGR2RGB)\n",
    "                display(Image.fromarray(input_image_rgb))\n",
    "                \n",
    "                image_height, image_width = input_image_bgr.shape[:2]\n",
    "                print(f\"Image dimensions: Width={image_width}, Height={image_height}\")\n",
    "\n",
    "                print(\"\\n--- Step B: Running YOLO Object Detection ---\")\n",
    "                detections, annotated_image_bgr = detect_objects_yolo(input_image_bgr.copy(), yolo_model)\n",
    "                \n",
    "                if detections:\n",
    "                    print(f\"Detected {len(detections)} objects.\")\n",
    "                else:\n",
    "                    print(\"No objects detected by YOLO.\")\n",
    "\n",
    "                print(\"\\n--- Annotated Image (YOLO Detections) ---\")\n",
    "                if annotated_image_bgr is not None:\n",
    "                    annotated_image_rgb = cv2.cvtColor(annotated_image_bgr, cv2.COLOR_BGR2RGB)\n",
    "                    display(Image.fromarray(annotated_image_rgb))\n",
    "                else:\n",
    "                    print(\"Annotated image is not available.\")\n",
    "\n",
    "                print(\"\\n--- Step C: Formatting Detections as JSON for LLM ---\")\n",
    "                detections_json_string = format_detections_as_json_for_llm(detections)\n",
    "                print(f\"Detections JSON for LLM:\\n{detections_json_string}\")\n",
    "\n",
    "                print(\"\\n--- Step D: Generating Text with DeepSeek ---\")\n",
    "                if detections_json_string == \"No objects were detected in the image.\":\n",
    "                    prompt_for_llm = f\"\"\"\n",
    "You are an AI assistant that describes images.\n",
    "The object detection process found no specific objects in an image that is {image_width} pixels wide and {image_height} pixels high.\n",
    "Please provide a very brief, general description acknowledging this, for example, \"The image appears to have no distinct objects detected.\"\n",
    "\"\"\"\n",
    "                else:\n",
    "                    prompt_for_llm = f\"\"\"\n",
    "You are an AI assistant that describes images based on a list of detected objects provided in JSON format.\n",
    "The image dimensions are {image_width} pixels wide and {image_height} pixels high.\n",
    "The following is a JSON list of objects detected in the image. Each object has:\n",
    "- 'class_name': The identified type of the object.\n",
    "- 'confidence': The model's confidence in this detection (a value between 0.0 and 1.0).\n",
    "- 'bbox': The bounding box coordinates [x1, y1, x2, y2], where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner of the object within the image dimensions mentioned above.\n",
    "Detected objects data:\n",
    "{detections_json_string}\n",
    "Based on this structured data, please generate a concise and natural language description of the scene.\n",
    "Try to infer relationships between objects and their general locations (e.g., \"left side\", \"center\", \"behind another object\")\n",
    "rather than just listing the raw bounding box coordinates.\n",
    "Focus on creating a human-like, coherent description of what the image likely contains.\n",
    "\n",
    "Here is an example of a natural language description:\n",
    "Input image: girl and dog sitting on a park bench reading a book\n",
    "Output description:\n",
    "\"In the central area of the park, a young woman (98% confidence) is sitting on a bench intently reading a book.She is accompanied by a golden retriever (92% confidence) lying quietly to her left.A vacant chair (87% confidence) can be seen in the lower right corner of the image, and the entire scene appears peaceful and welcoming.\"\n",
    "\"\"\"\n",
    "                print(f\"\\nPrompt being sent to DeepSeek:\\n{prompt_for_llm}\")\n",
    "                natural_language_description = generate_text_with_deepseek(prompt_for_llm)\n",
    "\n",
    "                print(\"\\n--- Step E: Output Natural Language Description ---\")\n",
    "                print(natural_language_description)\n",
    "\n",
    "            except FileNotFoundError as e_file:\n",
    "                print(f\"Error in file mode: {e_file}\")\n",
    "            except Exception as e_file_main:\n",
    "                print(f\"An unexpected error occurred in file mode: {e_file_main}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "    # --- CAMERA MODE ---\n",
    "    elif OPERATION_MODE == \"camera\":\n",
    "        print(f\"--- Running in CAMERA mode (Camera Index: {CAMERA_INDEX}) ---\")\n",
    "        cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open camera with index {CAMERA_INDEX}. Please check if the camera is connected and not in use.\")\n",
    "        else:\n",
    "            print(\"Camera opened successfully.\")\n",
    "            print(\"Instructions: Focus the OpenCV window. Press 'c' to capture and process the current frame. Press 'q' to quit.\")\n",
    "            \n",
    "            window_name_annotated = \"Annotated Frame (Press 'c' for next, 'q' to quit)\"\n",
    "            cv2.namedWindow(window_name_annotated, cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "            try:\n",
    "                while True:\n",
    "                    # Show a minimal live feed or just wait for key press\n",
    "                    # For simplicity, we'll capture on key press 'c'\n",
    "                    # To show a live feed, you'd read and imshow here before waitKey\n",
    "                    \n",
    "                    print(\"\\nWaiting for your command... (Focus OpenCV window: Press 'c' to capture, 'q' to quit)\")\n",
    "                    \n",
    "                    # Display a placeholder or the last annotated frame until 'c' is pressed\n",
    "                    # This requires a frame to be available. For the first run, we can show a black screen.\n",
    "                    # Or, better, just let the window be empty until first capture.\n",
    "                    # For continuous preview, you'd put cap.read() and cv2.imshow() here with waitKey(1)\n",
    "\n",
    "                    key = cv2.waitKey(0) & 0xFF # Wait indefinitely for a key press\n",
    "\n",
    "                    if key == ord('q'):\n",
    "                        print(\"Quit command received. Exiting camera mode.\")\n",
    "                        break\n",
    "                    elif key == ord('c'):\n",
    "                        print(\"Capture command received.\")\n",
    "                        ret, frame_bgr = cap.read()\n",
    "                        if not ret:\n",
    "                            print(\"Error: Could not read frame from camera. Exiting.\")\n",
    "                            break\n",
    "                        \n",
    "                        print(f\"\\n--- Processing new frame from camera ---\")\n",
    "                        \n",
    "                        # Display the captured frame (optional, as annotated one will be shown)\n",
    "                        # cv2.imshow(\"Captured Frame (Raw)\", frame_bgr) \n",
    "                        # cv2.waitKey(1)\n",
    "\n",
    "\n",
    "                        image_height, image_width = frame_bgr.shape[:2]\n",
    "                        print(f\"Frame dimensions: Width={image_width}, Height={image_height}\")\n",
    "\n",
    "                        print(\"\\n--- Step B: Running YOLO Object Detection ---\")\n",
    "                        detections, annotated_frame_bgr = detect_objects_yolo(frame_bgr.copy(), yolo_model)\n",
    "                        \n",
    "                        if detections:\n",
    "                            print(f\"Detected {len(detections)} objects.\")\n",
    "                        else:\n",
    "                            print(\"No objects detected by YOLO.\")\n",
    "\n",
    "                        print(\"\\n--- Annotated Frame (YOLO Detections) ---\")\n",
    "                        if annotated_frame_bgr is not None:\n",
    "                            cv2.imshow(window_name_annotated, annotated_frame_bgr)\n",
    "                        else:\n",
    "                            # If annotation failed, show the original captured frame\n",
    "                            cv2.imshow(window_name_annotated, frame_bgr) \n",
    "                            print(\"Annotated frame is not available, showing original captured frame.\")\n",
    "                        cv2.waitKey(1) # Allow window to update\n",
    "\n",
    "                        print(\"\\n--- Step C: Formatting Detections as JSON for LLM ---\")\n",
    "                        detections_json_string = format_detections_as_json_for_llm(detections)\n",
    "                        # print(f\"Detections JSON for LLM:\\n{detections_json_string}\") # Can be verbose in loop\n",
    "\n",
    "                        print(\"\\n--- Step D: Generating Text with DeepSeek ---\")\n",
    "                        if detections_json_string == \"No objects were detected in the image.\":\n",
    "                            prompt_for_llm = f\"\"\"\n",
    "You are an AI assistant that describes images.\n",
    "The object detection process found no specific objects in an image that is {image_width} pixels wide and {image_height} pixels high.\n",
    "Please provide a very brief, general description acknowledging this, for example, \"The image appears to have no distinct objects detected.\"\n",
    "\"\"\"\n",
    "                        else:\n",
    "                            prompt_for_llm = f\"\"\"\n",
    "You are an AI assistant that describes images based on a list of detected objects provided in JSON format.\n",
    "The image dimensions are {image_width} pixels wide and {image_height} pixels high.\n",
    "The following is a JSON list of objects detected in the image. Each object has:\n",
    "- 'class_name': The identified type of the object.\n",
    "- 'confidence': The model's confidence in this detection (a value between 0.0 and 1.0).\n",
    "- 'bbox': The bounding box coordinates [x1, y1, x2, y2], where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner of the object within the image dimensions mentioned above.\n",
    "Detected objects data:\n",
    "{detections_json_string}\n",
    "Based on this structured data, please generate a concise and natural language description of the scene.\n",
    "Try to infer relationships between objects and their general locations (e.g., \"left side\", \"center\", \"behind another object\")\n",
    "rather than just listing the raw bounding box coordinates.\n",
    "Focus on creating a human-like, coherent description of what the image likely contains.\n",
    "\n",
    "Here is an example of a natural language description:\n",
    "Input image: girl and dog sitting on a park bench reading a book\n",
    "Output description:\n",
    "\"In the central area of the park, a young woman (98% confidence) is sitting on a bench intently reading a book.She is accompanied by a golden retriever (92% confidence) lying quietly to her left.A vacant chair (87% confidence) can be seen in the lower right corner of the image, and the entire scene appears peaceful and welcoming.\"\n",
    "\"\"\"\n",
    "                        # print(f\"\\nPrompt being sent to DeepSeek:\\n{prompt_for_llm}\") # Can be verbose\n",
    "                        natural_language_description = generate_text_with_deepseek(prompt_for_llm)\n",
    "\n",
    "                        print(\"\\n--- Step E: Output Natural Language Description ---\")\n",
    "                        print(natural_language_description)\n",
    "                        print(\"-\" * 70) # Separator for next iteration\n",
    "                    else:\n",
    "                        # Optional: Handle other key presses if needed\n",
    "                        print(f\"Key '{chr(key)}' pressed. Press 'c' to capture or 'q' to quit.\")\n",
    "                        pass \n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Camera mode interrupted by user (Ctrl+C).\")\n",
    "            except Exception as e_cam_main:\n",
    "                print(f\"An unexpected error occurred in camera mode: {e_cam_main}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            finally:\n",
    "                if 'cap' in locals() and cap.isOpened():\n",
    "                    cap.release()\n",
    "                    print(\"Camera released.\")\n",
    "                cv2.destroyAllWindows()\n",
    "                print(\"OpenCV windows closed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image2text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
