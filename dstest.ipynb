{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6674bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "库已导入，Ollama 配置已加载。\n",
      "Ollama API URL: http://localhost:11434/api/generate\n",
      "将要使用的模型: deepseek-r1:8b\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Ollama API 配置\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"  # Ollama 'generate' API 的默认端点\n",
    "\n",
    "# 重要: 请将 'your-deepseek-model-name' 替换为您在 Ollama 中实际使用的 DeepSeek 模型名称。\n",
    "# 您可以通过在终端运行 `ollama list` 来找到它。\n",
    "# 例如: \"deepseek-coder\", \"deepseek-llm\", \"deepseek-llm:7b-chat\" 等。\n",
    "MODEL_NAME = \"deepseek-r1:8b\"\n",
    "# 示例: MODEL_NAME = \"deepseek-llm\"\n",
    "\n",
    "print(\"库已导入，Ollama 配置已加载。\")\n",
    "print(f\"Ollama API URL: {OLLAMA_API_URL}\")\n",
    "print(f\"将要使用的模型: {MODEL_NAME}\")\n",
    "\n",
    "if MODEL_NAME == \"your-deepseek-model-name\":\n",
    "    print(\"\\n⚠️ 警告: 请务必将上面代码中的 'MODEL_NAME' 修改为您在 Ollama 中实际的 DeepSeek 模型名称!\")\n",
    "    print(\"   您可以通过在终端运行 `ollama list` 来查看可用的模型。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d31fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 'query_ollama_deepseek' 已定义。\n"
     ]
    }
   ],
   "source": [
    "def query_ollama_deepseek(prompt_text, model_name=MODEL_NAME, ollama_url=OLLAMA_API_URL):\n",
    "    \"\"\"\n",
    "    向通过 Ollama 托管的 DeepSeek 模型发送提示并返回其响应。\n",
    "\n",
    "    参数:\n",
    "        prompt_text (str): 要发送给模型的提示文本。\n",
    "        model_name (str): 在 Ollama 中的模型名称。\n",
    "        ollama_url (str): Ollama 'generate' API 的 URL。\n",
    "\n",
    "    返回:\n",
    "        str: 模型的响应文本，或者在出错时返回错误信息。\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt_text,\n",
    "        \"stream\": False  # 设置为 False 以获取完整响应；True 则为流式响应\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    print(f\"\\n正在向模型 '{model_name}' 发送提示...\")\n",
    "    try:\n",
    "        response = requests.post(ollama_url, data=json.dumps(payload), headers=headers, timeout=120) # 设置120秒超时\n",
    "        response.raise_for_status()  # 如果发生 HTTP 错误 (4xx 或 5xx)，则抛出异常\n",
    "\n",
    "        response_data = response.json()\n",
    "        \n",
    "        # 'response' 字段通常包含 Ollama /api/generate 端点的完整回复\n",
    "        if \"response\" in response_data:\n",
    "            return response_data[\"response\"].strip()\n",
    "        else:\n",
    "            return f\"错误: 在 API 返回中未找到 'response' 键。完整返回: {response_data}\"\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return f\"错误: 无法连接到 Ollama 服务于 {ollama_url}。请确认 Ollama 是否正在运行。\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return f\"错误: 请求超时。模型可能需要更长时间来处理该提示，或者 Ollama 服务无响应。\"\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return f\"错误:发生 HTTP 错误: {e}。响应内容: {e.response.text if e.response else '无响应体'}\"\n",
    "    except json.JSONDecodeError:\n",
    "        return f\"错误: 无法解码来自 Ollama 的 JSON 响应。响应文本: {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"错误: 发生未知错误: {e}\"\n",
    "\n",
    "print(\"函数 'query_ollama_deepseek' 已定义。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26f339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试提示: \"你好，DeepSeek！请用中文简单介绍一下你自己以及你的能力。\"\n",
      "\n",
      "正在向模型 'deepseek-r1:8b' 发送提示...\n",
      "\n",
      "=============== 模型响应 ===============\n",
      "<think>\n",
      "嗯，用户让我用中文简单介绍自己和能力。这是一个非常基础的请求，可能来自新接触AI助手的人。\n",
      "\n",
      "用户大概刚打开聊天界面，想先了解这个工具能做什么。ta需要的是清晰、简洁且全面的能力说明，最好带点亲切感让对话更自然。考虑到中文环境下的使用习惯，“小深”这种昵称会比冷冰冰的“DeepSeek”更容易被接受。\n",
      "\n",
      "应该突出几个关键点：身份背景要透明但不过度营销（只说公司名不提融资细节），能力范围要全面覆盖主要功能（从聊天到专业领域再到工具支持），同时强调实用性而非技术原理。用emoji可以增加视觉友好度，但中文用户更习惯直接明了的说明。\n",
      "\n",
      "需要特别注意避免让用户产生“全能AI”的误解，所以要在列举能力和最后加上局限性的提醒。语气上要像朋友介绍功能那样自然，比如“你可以把我当成”这种说法就比官方说明书式更有温度。\n",
      "</think>\n",
      "你好呀！我是 DeepSeek-R1，一个由深度求索公司开发的人工智能助手🧠。我的主要任务是用自然、贴心的方式帮助你解答问题、整理资料、头脑风暴、翻译润色等等。\n",
      "\n",
      "你可以把我当成一位知识渊博又能耐心倾听的聊天伙伴，无论你是学生、职场人士还是创作者，我都能尽力帮你：\n",
      "\n",
      "💡 日常问答：生活中的各种疑惑都可以问我  \n",
      "📚 学习辅导：查资料、解题、写论文我都行  \n",
      "💼 工作助手：分析文档、整理报告、处理数据也轻松应对  \n",
      "✍️ 内容创作：写作灵感没啦？不急，我来帮你续上  \n",
      "\n",
      "而且我还支持中文对话（当然也能说英文），理解复杂上下文，并且可以读取你上传的文件内容进行分析。不过我也不是万能的，对于特别专业、时效性极强或需要主观判断的事情，可能还需要更多人的帮助。\n",
      "\n",
      "总之，我会尽我所能为你提供靠谱又温暖的帮助！🥰\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# 检查模型名称是否已更新\n",
    "if MODEL_NAME == \"your-deepseek-model-name\":\n",
    "    print(\"请先在第一个单元格中更新 'MODEL_NAME' 为您在 Ollama 中实际的 DeepSeek 模型名称，然后再运行此单元格。\")\n",
    "else:\n",
    "    # 在这里输入您想测试的提示\n",
    "    test_prompt = \"你好，DeepSeek！请用中文简单介绍一下你自己以及你的能力。\"\n",
    "    # 其他测试提示示例:\n",
    "    # test_prompt = \"Hello DeepSeek! Can you write a short story about a friendly robot?\"\n",
    "    # test_prompt = \"用 Python 写一个函数来计算斐波那契数列的第n项。\" # 如果是代码模型\n",
    "\n",
    "    print(f\"测试提示: \\\"{test_prompt}\\\"\")\n",
    "    \n",
    "    # 调用函数获取模型响应\n",
    "    model_response = query_ollama_deepseek(test_prompt)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*15 + \" 模型响应 \" + \"=\"*15)\n",
    "    print(model_response)\n",
    "    print(\"=\"* (30 + len(\" 模型响应 \")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image2text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
